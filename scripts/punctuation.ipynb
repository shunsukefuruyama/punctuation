{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DistilBERT_Arch(nn.Module):\n",
    "    def __init__(self, distilbert):\n",
    "        super().__init__()\n",
    "        self.distilbert = distilbert\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 4)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input_ids, mask):        \n",
    "        cls_hs = self.distilbert(input_ids, attention_mask=mask)[0]\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T17:07:24.025180Z",
     "start_time": "2021-04-23T17:07:24.022171Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/Development/punctuation/data/transcripts.csv\")\n",
    "# len(df): 2467\n",
    "\n",
    "# train:val:test = 0.8:0.1:0.1\n",
    "num_train = int(len(df)*0.8)+2 # 0.8 -> 0.01\n",
    "num_val = int(len(df)*0.1) # 0.1 -> 0.01\n",
    "num_test = int(len(df)*0.1) # 0.1 -> 0.01\n",
    "\n",
    "# assign indices\n",
    "id_all = np.random.choice(len(df), len(df), replace=False)\n",
    "id_train = id_all[0:num_train]\n",
    "id_val = id_all[num_train : num_val+num_train]\n",
    "id_test = id_all[num_val+num_train : num_val+num_train+num_test]\n",
    "\n",
    "# actual split\n",
    "train_set = df.iloc[id_train]\n",
    "val_set = df.iloc[id_val]\n",
    "test_set = df.iloc[id_test]\n",
    "\n",
    "# remove transcripts containing ♫\n",
    "train_set = train_set[~train_set['transcript'].str.contains('♫')]\n",
    "val_set = val_set[~val_set['transcript'].str.contains('♫')]\n",
    "test_set = test_set[~test_set['transcript'].str.contains('♫')]\n",
    "\n",
    "def data_prep(data_set):\n",
    "\n",
    "    # Dataset Cleanup\n",
    "    data_set = data_set.drop('url',axis=1)\n",
    "    data_set = data_set['transcript']\n",
    "    data_set = data_set.str.replace(\"\\(.*?\\)\", \" \")\\\n",
    "    .str.replace(\"\\[.*?\\]\", \" \")\\\n",
    "    .str.replace(\";\", \". \")\\\n",
    "    .str.replace(\":\", \". \")\\\n",
    "    .str.replace('\"', ' ')\\\n",
    "    .str.replace('!', '. ')\\\n",
    "    .str.replace(\" — (?=[a-z])\", \", \")\\\n",
    "    .str.replace(\" — (?=[A-Z])\", \". \")\\\n",
    "    .str.replace(\"(?<=[a-z])\\.(?=[A-Z])\", \". \")\\\n",
    "    .str.replace(\"(?<=[a-z])\\?(?=[A-Z])\", \". \")\\\n",
    "    .str.replace(\"(?<= )'(?=[a-zA-Z])\", \" \")\\\n",
    "    .str.replace(\"(?<=[a-z])\\'(?= )\", \" \")\\\n",
    "    .str.replace(\"\\'(?= )\", \" \")\\\n",
    "    .str.replace(\" — \", \" \")\\\n",
    "    .str.replace('\\.+', '.')\\\n",
    "    .str.replace(' +', ' ')\\\n",
    "    .str.lower()\n",
    "    # hyphens are hard to handle. for now sentences like below still have an issue:\n",
    "    # one - on - one tutoring works best so that's what we tried to emulate like with me and my mom even though we knew it would be one - on - thousands \n",
    "\n",
    "    temp_list_1 = []\n",
    "    for sentences in data_set:\n",
    "        temp_list_1 += re.split('(?<=\\.)|(?<=\\?)',sentences)\n",
    "\n",
    "    temp_list_2 = []\n",
    "    for item in temp_list_1:\n",
    "        temp_list_2.append(re.sub('^ ','',item))\n",
    "\n",
    "    temp_list_3 = []\n",
    "    for s in temp_list_2:\n",
    "        try:\n",
    "            if s[-1] == \".\":\n",
    "                temp_list_3.append(s)\n",
    "            elif s[-1] == \"?\":\n",
    "                temp_list_3.append(s)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    del data_set\n",
    "    del temp_list_1\n",
    "    del temp_list_2\n",
    "\n",
    "    total_words = 0\n",
    "    combined_text = \"\"\n",
    "    outer_list = []\n",
    "\n",
    "    # create outer_list, a list of sentences that don't go beyond 400 words\n",
    "    for s in temp_list_3:\n",
    "        if total_words + len(word_tokenize(s)) < 400:\n",
    "            combined_text += (s + \" \")\n",
    "            total_words += len(word_tokenize(s))\n",
    "        else:\n",
    "            outer_list.append(combined_text)\n",
    "            combined_text = \"\"\n",
    "            total_words = 0        \n",
    "\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    encoded_data_data = tokenizer.batch_encode_plus(outer_list, max_length=450, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "    punc_mask_outer = []\n",
    "    ids_no_punc_outer = []\n",
    "    attention_mask_outer = []\n",
    "\n",
    "    for j in range(len(encoded_data_data['input_ids'])):\n",
    "\n",
    "        # punctuation mask for sentences\n",
    "        punc_mask = []\n",
    "        for i in encoded_data_data['input_ids'][j]:\n",
    "            if i == 1012:\n",
    "                punc_mask.pop()\n",
    "                punc_mask.append(1) # period\n",
    "            elif i == 1029:\n",
    "                punc_mask.pop()\n",
    "                punc_mask.append(2) # question mark\n",
    "            elif i == 1010:\n",
    "                punc_mask.pop()\n",
    "                punc_mask.append(3) # comma\n",
    "            else:\n",
    "                punc_mask.append(0)\n",
    "        punc_mask_outer.append(torch.tensor(punc_mask))\n",
    "\n",
    "        # sentences converted to word ids excluding punctuations\n",
    "        # len(punc_mask) should be the same as len(ids_no_punc)\n",
    "        ids_no_punc = []\n",
    "        for i in encoded_data_data['input_ids'][j]:\n",
    "            if i == 1012:\n",
    "                pass\n",
    "            elif i == 1029:\n",
    "                pass\n",
    "            elif i == 1010:\n",
    "                pass\n",
    "            else:\n",
    "                ids_no_punc.append(i)\n",
    "        ids_no_punc_outer.append(torch.tensor(ids_no_punc))\n",
    "\n",
    "        # attention_mask with subwords set to 0 except for the last one\n",
    "        attention_mask = []\n",
    "        first_hash = True\n",
    "        for i in encoded_data_data['input_ids'][j]:\n",
    "            if (i == 101 or i == 102 or i == 0): # CLS, SEP, PAD\n",
    "                attention_mask.append(0)\n",
    "            elif (i == 1029 or i == 1010 or i == 1012):\n",
    "                pass\n",
    "            else:\n",
    "                if re.match(r'^##', tokenizer.decode([i])):         \n",
    "                    if first_hash == True:\n",
    "                        attention_mask.pop()\n",
    "                        attention_mask.append(0)\n",
    "                        first_hash == False\n",
    "                    attention_mask.append(1)\n",
    "                else:\n",
    "                    if first_hash == False:\n",
    "                        attention_mask.pop()\n",
    "                    attention_mask.append(1)                \n",
    "        attention_mask_outer.append(torch.tensor(attention_mask))\n",
    "\n",
    "    # figure out max length so that PADs can be added till it reaches max\n",
    "    token_lengths = []\n",
    "    for i in range(len(punc_mask_outer)):\n",
    "        token_lengths.append(len(punc_mask_outer[i]))\n",
    "    token_length_max = np.max(token_lengths)\n",
    "\n",
    "    for i in range(len(punc_mask_outer)):\n",
    "        # add PAD again because length is not equal after removing punctuations\n",
    "        zeros = [0] * (token_length_max - len(punc_mask_outer[i]))\n",
    "\n",
    "        punc_mask = torch.cat((punc_mask_outer[i], torch.tensor(zeros)), 0)\n",
    "        ids_no_punc = torch.cat((ids_no_punc_outer[i], torch.tensor(zeros)), 0)\n",
    "        attention_mask = torch.cat((attention_mask_outer[i], torch.tensor(zeros)), 0)\n",
    "\n",
    "        if i != 0:\n",
    "            pass\n",
    "            punc_mask_outer_adjusted = torch.cat((punc_mask_outer_adjusted, punc_mask.view(1,-1)),0)\n",
    "            ids_no_punc_outer_adjusted = torch.cat((ids_no_punc_outer_adjusted, ids_no_punc.view(1,-1)),0)\n",
    "            attention_mask_outer_adjusted = torch.cat((attention_mask_outer_adjusted, attention_mask.view(1,-1)),0)\n",
    "        else:\n",
    "            punc_mask_outer_adjusted = punc_mask.view(1,-1)\n",
    "            ids_no_punc_outer_adjusted = ids_no_punc.view(1,-1)\n",
    "            attention_mask_outer_adjusted = attention_mask.view(1,-1)\n",
    "            \n",
    "    return ids_no_punc_outer_adjusted, attention_mask_outer_adjusted, punc_mask_outer_adjusted, punc_mask_outer, tokenizer\n",
    "    \n",
    "tarin_set = data_prep(train_set)\n",
    "val_set = data_prep(val_set)\n",
    "test_set = data_prep(test_set)\n",
    "    \n",
    "with open('train_set.pickle', 'wb') as f:\n",
    "    pickle.dump(tarin_set, f) \n",
    "\n",
    "with open('val_set.pickle', 'wb') as f:\n",
    "    pickle.dump(val_set, f) \n",
    "\n",
    "with open('test_set.pickle', 'wb') as f:\n",
    "    pickle.dump(test_set, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Val with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T18:00:35.978821Z",
     "start_time": "2021-04-23T17:59:57.454603Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(f'Prep started: {current_time}', flush=True)\n",
    "print()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with open('train_set.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "ids_no_punc_outer_adjusted, attention_mask_outer_adjusted, punc_mask_outer_adjusted, punc_mask_outer, tokenizer_train = data\n",
    "\n",
    "with open('val_set.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "ids_no_punc_outer_adjusted_val, attention_mask_outer_adjusted_val, punc_mask_outer_adjusted_val, punc_mask_outer_val, tokenizer_val = data\n",
    "    \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "dataset_train = TensorDataset(ids_no_punc_outer_adjusted, attention_mask_outer_adjusted, punc_mask_outer_adjusted)\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "\n",
    "distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                    num_labels=4,  \n",
    "                                                    output_attentions=False,\n",
    "                                                    output_hidden_states=False)\n",
    "model = DistilBERT_Arch(distilbert)\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr=1e-5,\n",
    "                 eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps=0,\n",
    "                                           num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "# prep for class_weights\n",
    "for i, p in enumerate(punc_mask_outer):    \n",
    "    if i != 0:\n",
    "        punc_cat = torch.cat((punc_cat, p), dim=0)\n",
    "    else:\n",
    "        punc_cat = p\n",
    "        \n",
    "class_weights = compute_class_weight('balanced', np.unique(punc_cat), punc_cat.numpy())\n",
    "#TODO: haven't considered attention_mask yet!!\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "cross_entropy = nn.NLLLoss(weight=weights)\n",
    "\n",
    "patience_cnt = 0\n",
    "prev_f1 = 0\n",
    "\n",
    "param_list = []\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(f'Training started: {current_time}', flush=True)\n",
    "print()\n",
    "\n",
    "val_f1_list = []\n",
    "val_f1_micro_list = []\n",
    "val_f1_macro_list = []\n",
    "val_f1_weighted_list = []\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%m%d%H%M\")\n",
    "\n",
    "folder = './val_results_' + current_time\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    if patience_cnt <= 2:\n",
    "\n",
    "        loss_total = 0\n",
    "\n",
    "        preds_masked_all = torch.tensor([0]).to(device)\n",
    "        labels_masked_all = torch.tensor([0]).to(device)\n",
    "        \n",
    "        for batch in dataloader_train:\n",
    "\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "\n",
    "            batch = [b.to(device) for b in batch]\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            outputs = model(batch[0].to(torch.long), batch[1].to(torch.long))\n",
    "            \n",
    "            loss = cross_entropy(outputs.to(torch.float32).view(-1, 4), batch[2].to(torch.long).view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            loss_total += loss         \n",
    "                \n",
    "            preds = torch.argmax(outputs, axis=2)\n",
    "            attention_masks = batch[1].to(torch.bool)\n",
    "            labels = batch[2]\n",
    "\n",
    "            preds_masked = torch.masked_select(preds, attention_masks)\n",
    "            labels_masked = torch.masked_select(labels, attention_masks)\n",
    "            \n",
    "            preds_masked_all = torch.cat([preds_masked_all, preds_masked])\n",
    "            labels_masked_all = torch.cat([labels_masked_all, labels_masked])\n",
    "                           \n",
    "        loss = loss_total/len(dataloader_train)\n",
    "        acc = (preds_masked_all == labels_masked_all).sum() / len(preds_masked_all)        \n",
    "\n",
    "        print(f'epoch: {epoch+1}, tr_loss: {loss.item():.3f}, tr_acc: {acc:.3f}', flush=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            dataset_val = TensorDataset(ids_no_punc_outer_adjusted_val, attention_mask_outer_adjusted_val, punc_mask_outer_adjusted_val)\n",
    "            dataloader_val = DataLoader(dataset_val, sampler=RandomSampler(dataset_val), batch_size=batch_size)\n",
    "            \n",
    "            preds_masked_all = torch.tensor([0]).to(device)\n",
    "            labels_masked_all = torch.tensor([0]).to(device)\n",
    "                        \n",
    "            for val_batch in dataloader_val:\n",
    "\n",
    "                val_batch = [b.to(device) for b in val_batch]\n",
    "                val_outputs = model(val_batch[0].to(torch.long), val_batch[1].to(torch.long))\n",
    "            \n",
    "                preds = torch.argmax(val_outputs, axis=2)\n",
    "                attention_masks = val_batch[1].to(torch.bool)\n",
    "                labels = val_batch[2]\n",
    "\n",
    "                preds_masked = torch.masked_select(preds, attention_masks)\n",
    "                labels_masked = torch.masked_select(labels, attention_masks)\n",
    "\n",
    "                preds_masked_all = torch.cat([preds_masked_all, preds_masked])  \n",
    "                labels_masked_all = torch.cat([labels_masked_all, labels_masked])\n",
    "            \n",
    "            val_acc = (preds_masked_all == labels_masked_all).sum() / len(preds_masked_all)        \n",
    "    \n",
    "            preds_masked_all = preds_masked_all.to('cpu').numpy()\n",
    "            labels_masked_all = labels_masked_all.to('cpu').numpy()\n",
    "    \n",
    "            val_f1 = f1_score(labels_masked_all, preds_masked_all, average=None)\n",
    "            val_f1_micro = f1_score(labels_masked_all, preds_masked_all, average='micro')\n",
    "            val_f1_macro = f1_score(labels_masked_all, preds_masked_all, average='macro')\n",
    "            val_f1_weighted = f1_score(labels_masked_all, preds_masked_all, average='weighted')\n",
    "            \n",
    "            val_f1_list.append(val_f1)\n",
    "            val_f1_micro_list.append(val_f1_micro)\n",
    "            val_f1_macro_list.append(val_f1_macro)\n",
    "            val_f1_weighted_list.append(val_f1_weighted)\n",
    "                        \n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(f'Epoch complete: {current_time}', flush=True)\n",
    "            \n",
    "            print(f'val_acc: {val_acc:.4f}', flush=True)\n",
    "            print(f'val_f1: {val_f1}', flush=True)\n",
    "            print(f'val_f1_micro: {val_f1_micro:.4f}', flush=True)\n",
    "            print(f'val_f1_macro: {val_f1_macro:.4f}', flush=True)\n",
    "            print(f'val_f1_weighted: {val_f1_weighted:.4f}', flush=True)\n",
    "            print()\n",
    "                               \n",
    "            if prev_f1 >= val_f1_macro:\n",
    "                patience_cnt += 1\n",
    "            else:\n",
    "                patience_cnt = 0\n",
    "            \n",
    "            prev_f1 = val_f1_macro\n",
    "            param_list.append(model)\n",
    "                \n",
    "            torch.save(param_list, folder + '/distilbert_result.pt')\n",
    "            \n",
    "            f = open(folder + '/val_f1_list.txt', 'wb')\n",
    "            pickle.dump(val_f1_list, f)\n",
    "            \n",
    "            f = open(folder + '/val_f1_micro_list.txt', 'wb')\n",
    "            pickle.dump(val_f1_micro_list, f)\n",
    "            \n",
    "            f = open(folder + '/val_f1_macro_list.txt', 'wb')\n",
    "            pickle.dump(val_f1_macro_list, f)\n",
    "\n",
    "            f = open(folder + '/val_f1_weighted_list.txt', 'wb')\n",
    "            pickle.dump(val_f1_weighted_list, f)\n",
    "\n",
    "    else:\n",
    "        print(f'3rd consecutive degrades observed at epoch {epoch}. So the best is epoch {epoch-3}', flush=True)\n",
    "        break\n",
    "\n",
    "torch.save(param_list[-4], folder + '/distilbert_result.pt')\n",
    "        \n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(f'Completed: {current_time}', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/home/ubuntu/Development/punctuation/val_results_05060418\"\n",
    "\n",
    "file = open(folder + '/val_f1_list.txt', 'rb')\n",
    "val_f1 = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(folder + '/val_f1_micro_list.txt', 'rb')\n",
    "val_f1_micro = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(folder + '/val_f1_macro_list.txt', 'rb')\n",
    "val_f1_macro = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(folder + '/val_f1_weighted_list.txt', 'rb')\n",
    "val_f1_weighted = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax3 = fig.add_subplot(313)\n",
    "\n",
    "ax1.plot(range(1, len(val_f1_micro)+1), val_f1_micro)\n",
    "ax2.plot(range(1, len(val_f1_macro)+1), val_f1_macro)\n",
    "ax3.plot(range(1, len(val_f1_weighted)+1), val_f1_weighted)\n",
    "\n",
    "ax1.axvline(14, color=\"yellow\")\n",
    "ax2.axvline(14, color=\"yellow\")\n",
    "ax3.axvline(14, color=\"yellow\")\n",
    "\n",
    "ax1.set_title('val_f1_micro')\n",
    "ax2.set_title('val_f1_macro')\n",
    "ax3.set_title('val_f1_weighted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = [x[0] for x in val_f1]\n",
    "label_1 = [x[1] for x in val_f1]\n",
    "label_2 = [x[2] for x in val_f1]\n",
    "label_3 = [x[3] for x in val_f1]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "ax1 = fig.add_subplot(411)\n",
    "ax2 = fig.add_subplot(412)\n",
    "ax3 = fig.add_subplot(413)\n",
    "ax4 = fig.add_subplot(414)\n",
    "\n",
    "ax1.plot(range(1, len(label_0)+1), label_0)\n",
    "ax2.plot(range(1, len(label_1)+1), label_1)\n",
    "ax3.plot(range(1, len(label_2)+1), label_2)\n",
    "ax4.plot(range(1, len(label_3)+1), label_3)\n",
    "\n",
    "ax1.axvline(14, color=\"yellow\")\n",
    "ax2.axvline(14, color=\"yellow\")\n",
    "ax3.axvline(14, color=\"yellow\")\n",
    "ax4.axvline(14, color=\"yellow\")\n",
    "\n",
    "ax1.set_title('f1: non_punc')\n",
    "ax2.set_title('f1: period')\n",
    "ax3.set_title('f1: question')\n",
    "ax4.set_title('f1: comma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started: 12:47:02\n",
      "test_acc: 0.901\n",
      "test_f1: [0.95720808 0.72939145 0.46546088 0.5636657 ]\n",
      "test_f1_micro: 0.901\n",
      "test_f1_macro: 0.679\n",
      "test_f1_weighted: 0.913\n",
      "Completed: 12:47:24\n"
     ]
    }
   ],
   "source": [
    "folder = \"/home/ubuntu/Development/punctuation/val_results_05060418\"\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(f'Started: {current_time}')\n",
    "\n",
    "model = torch.load(folder + '/distilbert_result.pt')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad(): # is this necessary?\n",
    "\n",
    "    with open('/home/ubuntu/Development/punctuation/data/test_set.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    ids_no_punc_outer_adjusted_test, attention_mask_outer_adjusted_test, punc_mask_outer_adjusted_test, punc_mask_outer_test, tokenizer_test = data\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    batch_size = 32\n",
    "#     epochs = 19\n",
    "\n",
    "    dataset_test = TensorDataset(ids_no_punc_outer_adjusted_test, attention_mask_outer_adjusted_test, punc_mask_outer_adjusted_test)\n",
    "    dataloader_test = DataLoader(dataset_test, sampler=RandomSampler(dataset_test), batch_size=batch_size)\n",
    "\n",
    "#     test_nume = 0\n",
    "#     test_deno = 0\n",
    "\n",
    "#     test_preds_cat = []\n",
    "#     test_labels_cat = []\n",
    "    \n",
    "    preds_masked_all = torch.tensor([0]).to(device)\n",
    "    labels_masked_all = torch.tensor([0]).to(device)\n",
    "    \n",
    "    for test_batch in dataloader_test:\n",
    "\n",
    "#         test_batch = [b.to(device) for b in test_batch]\n",
    "#         test_outputs = model(test_batch[0].to(torch.long), test_batch[1].to(torch.long))\n",
    "\n",
    "#         for j in range(test_outputs.shape[0]):\n",
    "#         # for jth sample in a batch\n",
    "\n",
    "#             test_preds = np.argmax(test_outputs[j].to('cpu').detach().numpy(), axis=1)\n",
    "#             test_labels = test_batch[2].to(torch.long)[j].to('cpu').detach().numpy()\n",
    "\n",
    "#             # for ith token in a jth sample\n",
    "#             # if attention mask is not 0, check if predictinon matches label\n",
    "#             for i in range(len(test_batch[1][j])):\n",
    "#                 if test_batch[1][j][i] != 0:\n",
    "#                     if test_preds[i] == test_labels[i]:    \n",
    "#                         test_nume += 1\n",
    "                        \n",
    "#                     test_preds_cat.append(test_preds[i])\n",
    "#                     test_labels_cat.append(test_labels[i])\n",
    "                        \n",
    "#                     test_deno += 1\n",
    "\n",
    "#     test_preds_cat = np.array(test_preds_cat)\n",
    "#     test_labels_cat = np.array(test_labels_cat)\n",
    "\n",
    "        test_batch = [b.to(device) for b in test_batch]\n",
    "        test_outputs = model(test_batch[0].to(torch.long), test_batch[1].to(torch.long))\n",
    "\n",
    "        preds = torch.argmax(test_outputs, axis=2)\n",
    "        attention_masks = test_batch[1].to(torch.bool)\n",
    "        labels = test_batch[2]\n",
    "\n",
    "        preds_masked = torch.masked_select(preds, attention_masks)\n",
    "        labels_masked = torch.masked_select(labels, attention_masks)\n",
    "\n",
    "        preds_masked_all = torch.cat([preds_masked_all, preds_masked])  \n",
    "        labels_masked_all = torch.cat([labels_masked_all, labels_masked])\n",
    "\n",
    "    test_acc = (preds_masked_all == labels_masked_all).sum() / len(preds_masked_all)        \n",
    "\n",
    "    preds_masked_all = preds_masked_all.to('cpu').numpy()\n",
    "    labels_masked_all = labels_masked_all.to('cpu').numpy()\n",
    "    \n",
    "#     test_acc = test_nume/test_deno    \n",
    "#     test_f1 = f1_score(test_labels_cat, test_preds_cat, average=None)\n",
    "#     test_f1_micro = f1_score(test_labels_cat, test_preds_cat, average='micro')\n",
    "#     test_f1_macro = f1_score(test_labels_cat, test_preds_cat, average='macro')\n",
    "#     test_f1_weighted = f1_score(test_labels_cat, test_preds_cat, average='weighted')\n",
    "    \n",
    "    test_f1 = f1_score(labels_masked_all, preds_masked_all, average=None)\n",
    "    test_f1_micro = f1_score(labels_masked_all, preds_masked_all, average='micro')\n",
    "    test_f1_macro = f1_score(labels_masked_all, preds_masked_all, average='macro')\n",
    "    test_f1_weighted = f1_score(labels_masked_all, preds_masked_all, average='weighted')\n",
    "    \n",
    "    print(f'test_acc: {test_acc:.3f}', flush=True)\n",
    "    print(f'test_f1: {test_f1}', flush=True)\n",
    "    print(f'test_f1_micro: {test_f1_micro:.3f}', flush=True)\n",
    "    print(f'test_f1_macro: {test_f1_macro:.3f}', flush=True)\n",
    "    print(f'test_f1_weighted: {test_f1_weighted:.3f}', flush=True)\n",
    "    \n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(f'Completed: {current_time}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Development/punctuation'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluency",
   "language": "python",
   "name": "fluency"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
